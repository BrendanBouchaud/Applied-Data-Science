{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Black Scholes + Monte Carlo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MryZdNvkLU42"
      },
      "source": [
        "# **Applied Data Science in Finance**\n",
        "\n",
        "---\n",
        "\n",
        "In this notebook, we would like to apply data science on quantitative financial topics. We will apply neural network method on our option pricing tools. The notebook will cover from the option pricing tool to complicated quantitative models such as Black Scholes and Heston model. \n",
        "\n",
        "In this notebook, we use call option as an example to generate all the results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Dm1tUJCmLVsw"
      },
      "source": [
        "import scipy\n",
        "from scipy import stats\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import scipy.stats as ss\n",
        "from numpy import sqrt, exp\n",
        "from sklearn.model_selection import train_test_split\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers\n",
        "from tensorflow.keras import Model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uqLOMR94RJXZ"
      },
      "source": [
        "## 1.1 Analytical option pricing tool"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6TusSRpOOrRp"
      },
      "source": [
        "# Generate the cumulative distribution function \n",
        "cdf = stats.norm(0, 1).cdf\n",
        "\"\"\"\n",
        "  K is the strike price\n",
        "  S asset price\n",
        "  T Maturity \n",
        "  r risk free rate \n",
        "  q dividend rate\n",
        "  σ volatility\n",
        "\n",
        "\"\"\"\n",
        "\n",
        "# Define D1\n",
        "def d1(S, K, T, r, σ):\n",
        "    return (np.log(S / K) + ((r) + 0.5 * σ ** 2) * T) / (σ * np.sqrt(T))\n",
        "\n",
        "#Define D2\n",
        "def d2(d1,T,σ):\n",
        "    return d1- σ * np.sqrt(T)\n",
        "\n",
        "# Calls\n",
        "def call(S, K, T, r,d1,d2):\n",
        "    return S *np.exp(r*T)* cdf(d1) - K * np.exp(-r * T) * cdf(d2)\n",
        "\n",
        "#Puts\n",
        "def put(S, K, T, r,d1,d2):\n",
        "    return np.exp(-r * T) * K * cdf(-d2) - S *np.exp(r*T)* cdf(-d1)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CyZI-30SRj4Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "756a8d3e-af6a-44fc-828a-5a47a182348c"
      },
      "source": [
        "# Trial with the above defined formula\n",
        "S,K,T,r,q,σ= 100, 100, 1, 0.05, 0.0, 0.3\n",
        "d1 = d1(S, K, T, r, σ)\n",
        "d2 = d2(d1,T,σ)\n",
        "C = call(S, K, T, r,d1,d2)\n",
        "P = put(S, K, T, r,d1,d2)\n",
        "print(C);print(P)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "17.431861836422698\n",
            "7.427694648891695\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "tKSqfynnOvC7"
      },
      "source": [
        "# Create an input dataset\n",
        "spot_prices=[v for v in range(5,100,5)] # Generate a list starting with 5 and ending with 100 with an interval of 5\n",
        "strike_prices=[u for u in range(5,90,10)] # Generate a list starting with 5 and ending with 90 with an interval of 10\n",
        "interest_rates=[0.01,0.015,0.02,0.025]\n",
        "volatilities=[0.05,0.1,0.15,0.2,0.25,0.3]\n",
        "expiry_times=[7,14,21,28,35,42,49]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vo605tMSSDBI"
      },
      "source": [
        "From this step, we would like to apply the input dataset we create above in our analytical option tool, in order to generate a full dataset with the input and output. \n",
        "\n",
        "Then, we split the complete dataset into training and testing set. As default, the testing dataset will remain at 20% of the whole dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "THhOALjiPC4s"
      },
      "source": [
        "# Generate whole dataset and split it into training and testing\n",
        "call_option=[]\n",
        "for σ in volatilities:\n",
        "    for r in interest_rates:\n",
        "        for T in expiry_times:\n",
        "            for K in strike_prices:\n",
        "                for S in spot_prices:\n",
        "                    price=call(S, K, T, r,d1,d2)\n",
        "                \n",
        "                    call_option.append({\n",
        "                        'volatility':σ,\n",
        "                        'interest_rate':r,\n",
        "                        'expiry_time':T,\n",
        "                        'initial_price':S,\n",
        "                        'strike_price':K,\n",
        "                        'price':price\n",
        "                    })\n",
        "df=pd.DataFrame.from_dict(call_option)\n",
        "df.head()\n",
        "x=df[['volatility','interest_rate','expiry_time','initial_price','strike_price']].values\n",
        "y=df['price'].values\n",
        "x_train, x_val, y_train, y_val=train_test_split(x,y,test_size=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AytNgt-eSpSg"
      },
      "source": [
        "## 1.2 Analytical option pricing tool with neural network training\n",
        "After creating the analytical option pricing tool, we would like to apply neural network model to train our dataset. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49NvRAVrRuOY"
      },
      "source": [
        "# 2 hidden layer to predict an output of 1 dimension (price) with 5 dimension input\n",
        "def create_network(input_dim, output_dim, layer1_dim, layer2_dim):\n",
        "    inputs=tf.keras.Input(shape=(input_dim,))\n",
        "    encoded1=tf.keras.layers.Dense(layer1_dim,activation='relu')(inputs)\n",
        "    encoded2=tf.keras.layers.Dense(layer2_dim,activation='relu')(encoded1)\n",
        "    output=tf.keras.layers.Dense(output_dim)(encoded2)\n",
        "    model=tf.keras.Model(inputs,output)\n",
        "    return model"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "swo0jLdwTbnZ"
      },
      "source": [
        "We choose 200 as epochs, as we observe the val_loss starts to remain stable after 20 training. Therefore, we keep 200 as a threshold for the epochs. In terms of shuffle, we put it into \"False\" to keep a lower val_loss."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sAEoU5oCRu7O",
        "outputId": "7afaa206-b06c-44ba-b147-da01eb513f23"
      },
      "source": [
        "# Integrate inputs and train dataset\n",
        "input_dim = x_train.shape[1]\n",
        "output_dim = 1\n",
        "model = create_network(input_dim,output_dim, 200, 200)\n",
        "model.compile(optimizer='adam',loss='mean_squared_error')\n",
        "training=model.fit(x_train,y_train,epochs=200,batch_size=32,shuffle=False,validation_data=(x_val,y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "898/898 [==============================] - 2s 2ms/step - loss: 411.3624 - val_loss: 124.2580\n",
            "Epoch 2/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 265.4288 - val_loss: 130.0351\n",
            "Epoch 3/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 250.5447 - val_loss: 136.9921\n",
            "Epoch 4/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 241.2757 - val_loss: 145.1490\n",
            "Epoch 5/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 234.9283 - val_loss: 150.9789\n",
            "Epoch 6/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 229.5960 - val_loss: 149.6429\n",
            "Epoch 7/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 224.1866 - val_loss: 162.4435\n",
            "Epoch 8/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 218.4896 - val_loss: 160.1441\n",
            "Epoch 9/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 212.7361 - val_loss: 161.3345\n",
            "Epoch 10/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 206.7331 - val_loss: 159.6335\n",
            "Epoch 11/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 199.9830 - val_loss: 158.6857\n",
            "Epoch 12/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 193.2127 - val_loss: 156.5442\n",
            "Epoch 13/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 186.3363 - val_loss: 148.7014\n",
            "Epoch 14/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 177.9341 - val_loss: 148.0336\n",
            "Epoch 15/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 167.6295 - val_loss: 152.1052\n",
            "Epoch 16/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 156.0901 - val_loss: 143.2461\n",
            "Epoch 17/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 145.5539 - val_loss: 134.3982\n",
            "Epoch 18/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 129.8700 - val_loss: 277.5241\n",
            "Epoch 19/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 98.4422 - val_loss: 205.3751\n",
            "Epoch 20/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 86.9917 - val_loss: 74.6404\n",
            "Epoch 21/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 61.7338 - val_loss: 44.5327\n",
            "Epoch 22/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 47.1019 - val_loss: 29.3680\n",
            "Epoch 23/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 36.2509 - val_loss: 19.5919\n",
            "Epoch 24/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 24.8458 - val_loss: 16.8682\n",
            "Epoch 25/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 16.2853 - val_loss: 23.3479\n",
            "Epoch 26/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 18.4696 - val_loss: 15.9313\n",
            "Epoch 27/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 17.9615 - val_loss: 14.0756\n",
            "Epoch 28/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 16.0754 - val_loss: 8.7875\n",
            "Epoch 29/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 17.8895 - val_loss: 9.1123\n",
            "Epoch 30/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 17.2628 - val_loss: 10.4871\n",
            "Epoch 31/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 25.0869 - val_loss: 12.4955\n",
            "Epoch 32/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 21.8048 - val_loss: 24.3332\n",
            "Epoch 33/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 23.9537 - val_loss: 17.1754\n",
            "Epoch 34/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 22.8222 - val_loss: 11.2680\n",
            "Epoch 35/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 22.3166 - val_loss: 15.0526\n",
            "Epoch 36/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 15.9348 - val_loss: 12.5996\n",
            "Epoch 37/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 19.5977 - val_loss: 13.6076\n",
            "Epoch 38/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 21.2230 - val_loss: 10.9856\n",
            "Epoch 39/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 21.3214 - val_loss: 11.7032\n",
            "Epoch 40/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 21.2907 - val_loss: 12.4434\n",
            "Epoch 41/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 17.8054 - val_loss: 9.8663\n",
            "Epoch 42/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 22.3987 - val_loss: 9.3571\n",
            "Epoch 43/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 15.6748 - val_loss: 8.1534\n",
            "Epoch 44/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 20.3116 - val_loss: 9.7069\n",
            "Epoch 45/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 19.7431 - val_loss: 8.6817\n",
            "Epoch 46/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 15.4228 - val_loss: 7.5522\n",
            "Epoch 47/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 22.1833 - val_loss: 5.7118\n",
            "Epoch 48/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 10.0615 - val_loss: 8.0704\n",
            "Epoch 49/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 18.4201 - val_loss: 12.2730\n",
            "Epoch 50/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 26.2765 - val_loss: 25.7108\n",
            "Epoch 51/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 15.8776 - val_loss: 6.1259\n",
            "Epoch 52/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 13.2667 - val_loss: 10.0090\n",
            "Epoch 53/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 11.4465 - val_loss: 6.6585\n",
            "Epoch 54/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 15.7478 - val_loss: 9.8939\n",
            "Epoch 55/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 13.2513 - val_loss: 12.2396\n",
            "Epoch 56/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 12.6925 - val_loss: 6.5835\n",
            "Epoch 57/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 9.9613 - val_loss: 4.9598\n",
            "Epoch 58/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 19.1184 - val_loss: 6.5711\n",
            "Epoch 59/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 13.4760 - val_loss: 7.7927\n",
            "Epoch 60/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 20.4521 - val_loss: 5.5366\n",
            "Epoch 61/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 12.0857 - val_loss: 8.4179\n",
            "Epoch 62/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 9.9206 - val_loss: 8.1316\n",
            "Epoch 63/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 8.9255 - val_loss: 10.0584\n",
            "Epoch 64/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 17.1295 - val_loss: 4.7465\n",
            "Epoch 65/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 15.6063 - val_loss: 3.8181\n",
            "Epoch 66/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 16.6042 - val_loss: 5.8727\n",
            "Epoch 67/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 7.3673 - val_loss: 4.6352\n",
            "Epoch 68/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 17.1285 - val_loss: 5.0818\n",
            "Epoch 69/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 11.2798 - val_loss: 4.9503\n",
            "Epoch 70/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 17.5292 - val_loss: 6.1356\n",
            "Epoch 71/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 9.8995 - val_loss: 4.0230\n",
            "Epoch 72/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 17.7456 - val_loss: 6.4645\n",
            "Epoch 73/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 11.4453 - val_loss: 6.4502\n",
            "Epoch 74/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 6.2254 - val_loss: 7.7637\n",
            "Epoch 75/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 10.4535 - val_loss: 3.0712\n",
            "Epoch 76/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 7.1935 - val_loss: 1.9223\n",
            "Epoch 77/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 13.6042 - val_loss: 8.2456\n",
            "Epoch 78/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 9.5190 - val_loss: 10.3888\n",
            "Epoch 79/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 11.0386 - val_loss: 6.3820\n",
            "Epoch 80/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 15.5183 - val_loss: 5.5483\n",
            "Epoch 81/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 21.7392 - val_loss: 4.0156\n",
            "Epoch 82/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 7.7794 - val_loss: 5.5127\n",
            "Epoch 83/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 7.2895 - val_loss: 3.5804\n",
            "Epoch 84/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 5.7982 - val_loss: 5.2386\n",
            "Epoch 85/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 12.9162 - val_loss: 3.5603\n",
            "Epoch 86/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 4.4448 - val_loss: 3.5826\n",
            "Epoch 87/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 8.7439 - val_loss: 6.1628\n",
            "Epoch 88/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 9.5889 - val_loss: 3.0482\n",
            "Epoch 89/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 19.8269 - val_loss: 1.8584\n",
            "Epoch 90/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 7.0708 - val_loss: 1.9666\n",
            "Epoch 91/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 16.7450 - val_loss: 2.1024\n",
            "Epoch 92/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 6.0285 - val_loss: 1.2818\n",
            "Epoch 93/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 9.7428 - val_loss: 2.5688\n",
            "Epoch 94/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 4.9709 - val_loss: 1.9854\n",
            "Epoch 95/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 11.3935 - val_loss: 2.6943\n",
            "Epoch 96/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 6.3023 - val_loss: 1.4999\n",
            "Epoch 97/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 20.9777 - val_loss: 1.3730\n",
            "Epoch 98/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 6.4707 - val_loss: 2.3869\n",
            "Epoch 99/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 5.5218 - val_loss: 2.9499\n",
            "Epoch 100/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 7.9532 - val_loss: 1.7279\n",
            "Epoch 101/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 14.6368 - val_loss: 1.0651\n",
            "Epoch 102/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 9.1209 - val_loss: 4.7856\n",
            "Epoch 103/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 10.8255 - val_loss: 1.7361\n",
            "Epoch 104/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 11.0575 - val_loss: 5.1943\n",
            "Epoch 105/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 6.1938 - val_loss: 3.2660\n",
            "Epoch 106/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 21.9399 - val_loss: 2.3628\n",
            "Epoch 107/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 7.4104 - val_loss: 1.6160\n",
            "Epoch 108/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 5.0837 - val_loss: 2.3799\n",
            "Epoch 109/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 10.0548 - val_loss: 0.9603\n",
            "Epoch 110/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 4.4471 - val_loss: 2.8015\n",
            "Epoch 111/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 4.4087 - val_loss: 3.4275\n",
            "Epoch 112/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 4.0194 - val_loss: 7.2090\n",
            "Epoch 113/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 7.1307 - val_loss: 2.1349\n",
            "Epoch 114/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 8.5555 - val_loss: 2.1460\n",
            "Epoch 115/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 7.5913 - val_loss: 1.9372\n",
            "Epoch 116/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 5.8986 - val_loss: 1.0884\n",
            "Epoch 117/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 7.0246 - val_loss: 4.2565\n",
            "Epoch 118/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 8.4283 - val_loss: 8.5460\n",
            "Epoch 119/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 11.5930 - val_loss: 7.3939\n",
            "Epoch 120/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 12.1947 - val_loss: 2.3445\n",
            "Epoch 121/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 7.8955 - val_loss: 0.9013\n",
            "Epoch 122/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 5.5253 - val_loss: 8.2259\n",
            "Epoch 123/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 8.3898 - val_loss: 1.3920\n",
            "Epoch 124/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 6.1349 - val_loss: 1.4648\n",
            "Epoch 125/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 7.3674 - val_loss: 8.9197\n",
            "Epoch 126/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 9.0894 - val_loss: 4.1499\n",
            "Epoch 127/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 6.6281 - val_loss: 5.9201\n",
            "Epoch 128/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 11.8642 - val_loss: 7.1587\n",
            "Epoch 129/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 8.2422 - val_loss: 3.7845\n",
            "Epoch 130/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 7.2642 - val_loss: 0.5137\n",
            "Epoch 131/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 6.8006 - val_loss: 7.6756\n",
            "Epoch 132/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 12.1965 - val_loss: 8.4463\n",
            "Epoch 133/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 9.8004 - val_loss: 3.8094\n",
            "Epoch 134/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 12.9702 - val_loss: 1.9102\n",
            "Epoch 135/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 8.9188 - val_loss: 2.4112\n",
            "Epoch 136/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 9.8843 - val_loss: 1.7915\n",
            "Epoch 137/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 5.6958 - val_loss: 3.1958\n",
            "Epoch 138/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 6.6192 - val_loss: 6.8490\n",
            "Epoch 139/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 4.9260 - val_loss: 4.5878\n",
            "Epoch 140/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 9.7320 - val_loss: 2.0515\n",
            "Epoch 141/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 6.6169 - val_loss: 6.3043\n",
            "Epoch 142/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 9.8611 - val_loss: 4.1666\n",
            "Epoch 143/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 10.1911 - val_loss: 2.0580\n",
            "Epoch 144/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 6.2485 - val_loss: 0.9801\n",
            "Epoch 145/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 8.4039 - val_loss: 1.8213\n",
            "Epoch 146/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 4.2169 - val_loss: 2.4306\n",
            "Epoch 147/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 9.5535 - val_loss: 1.4754\n",
            "Epoch 148/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 14.0567 - val_loss: 1.4530\n",
            "Epoch 149/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 10.2546 - val_loss: 2.3884\n",
            "Epoch 150/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 10.0861 - val_loss: 3.5641\n",
            "Epoch 151/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 13.6502 - val_loss: 1.6898\n",
            "Epoch 152/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 9.4698 - val_loss: 1.4596\n",
            "Epoch 153/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 5.2527 - val_loss: 2.2904\n",
            "Epoch 154/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 10.8544 - val_loss: 4.1023\n",
            "Epoch 155/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 7.1151 - val_loss: 5.6770\n",
            "Epoch 156/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 8.8920 - val_loss: 1.0964\n",
            "Epoch 157/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 6.4033 - val_loss: 5.7514\n",
            "Epoch 158/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 9.0889 - val_loss: 1.7054\n",
            "Epoch 159/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 3.5953 - val_loss: 1.0307\n",
            "Epoch 160/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 8.7607 - val_loss: 12.2355\n",
            "Epoch 161/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 10.5635 - val_loss: 10.7376\n",
            "Epoch 162/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 9.2880 - val_loss: 0.8848\n",
            "Epoch 163/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 5.3298 - val_loss: 3.0334\n",
            "Epoch 164/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 4.3127 - val_loss: 2.3835\n",
            "Epoch 165/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 6.5657 - val_loss: 0.8294\n",
            "Epoch 166/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 6.1612 - val_loss: 3.3751\n",
            "Epoch 167/200\n",
            "898/898 [==============================] - 1s 1ms/step - loss: 6.3006 - val_loss: 1.8940\n",
            "Epoch 168/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 9.0735 - val_loss: 1.5451\n",
            "Epoch 169/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 3.3907 - val_loss: 2.1301\n",
            "Epoch 170/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 10.0658 - val_loss: 1.3535\n",
            "Epoch 171/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 3.4698 - val_loss: 10.1351\n",
            "Epoch 172/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 10.6493 - val_loss: 5.8174\n",
            "Epoch 173/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 8.2523 - val_loss: 0.5445\n",
            "Epoch 174/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 6.0624 - val_loss: 0.4366\n",
            "Epoch 175/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 7.4198 - val_loss: 0.7101\n",
            "Epoch 176/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 4.6969 - val_loss: 14.6002\n",
            "Epoch 177/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 5.3737 - val_loss: 0.3219\n",
            "Epoch 178/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 6.9300 - val_loss: 6.4698\n",
            "Epoch 179/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 11.5940 - val_loss: 6.0507\n",
            "Epoch 180/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 7.0304 - val_loss: 3.6715\n",
            "Epoch 181/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 6.3555 - val_loss: 3.2678\n",
            "Epoch 182/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 4.1344 - val_loss: 3.7817\n",
            "Epoch 183/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 9.4999 - val_loss: 1.5251\n",
            "Epoch 184/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 4.0851 - val_loss: 2.9158\n",
            "Epoch 185/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 5.2108 - val_loss: 1.1972\n",
            "Epoch 186/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 6.1299 - val_loss: 3.4107\n",
            "Epoch 187/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 5.7384 - val_loss: 2.6247\n",
            "Epoch 188/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 8.9064 - val_loss: 0.3675\n",
            "Epoch 189/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 2.8280 - val_loss: 3.1877\n",
            "Epoch 190/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 5.5632 - val_loss: 0.2892\n",
            "Epoch 191/200\n",
            "898/898 [==============================] - 2s 2ms/step - loss: 2.5035 - val_loss: 0.5922\n",
            "Epoch 192/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 7.4801 - val_loss: 8.8732\n",
            "Epoch 193/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 10.1357 - val_loss: 5.8952\n",
            "Epoch 194/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 8.8036 - val_loss: 6.1930\n",
            "Epoch 195/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 8.3205 - val_loss: 1.3337\n",
            "Epoch 196/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 2.9814 - val_loss: 1.4547\n",
            "Epoch 197/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 4.3310 - val_loss: 0.8495\n",
            "Epoch 198/200\n",
            "898/898 [==============================] - 2s 2ms/step - loss: 4.5476 - val_loss: 0.5193\n",
            "Epoch 199/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 6.7558 - val_loss: 3.7168\n",
            "Epoch 200/200\n",
            "898/898 [==============================] - 1s 2ms/step - loss: 9.7332 - val_loss: 1.4557\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wWb4EhljWImT",
        "outputId": "914ae715-7853-435e-fd8f-203c6c0334e8"
      },
      "source": [
        "data_test=[]\n",
        "num=1000\n",
        "\n",
        "for n in range(num):\n",
        "  σ = np.random.uniform(low=volatilities[0],high=volatilities[-1])\n",
        "  r=np.random.uniform(low=interest_rates[0],high=interest_rates[-1])\n",
        "  T=np.random.uniform(low=expiry_times[0],high=expiry_times[-1])\n",
        "  S=np.random.uniform(low=spot_prices[0],high=spot_prices[-1])\n",
        "  K=np.random.uniform(low=strike_prices[0],high=strike_prices[-1])\n",
        "\n",
        "  price=call(S, K, T, r,d1,d2)\n",
        "                \n",
        "  data_test.append({\n",
        "        'volatility':σ,\n",
        "        'interest_rate':r,\n",
        "        'expiry_time':T,\n",
        "        'initial_price':S,\n",
        "        'strike_price':K,\n",
        "        'price':price\n",
        "      })\n",
        "  \n",
        "df_test=pd.DataFrame.from_dict(data_test)\n",
        "df_test.head()\n",
        "\n",
        "X_test=df_test[['volatility','interest_rate','expiry_time','initial_price','strike_price']].values\n",
        "y_test=df_test['price'].values\n",
        "\n",
        "loss = model.evaluate(X_test, y_test, verbose=0)\n",
        "\n",
        "print(\"average absolute error on test data is {}\".format(np.sqrt(loss)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average absolute error on test data is 0.915320451883592\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFyeU2KYXjm5"
      },
      "source": [
        "## 2. Black Scholes Option Pricing via Monte Carlo paths simulation\n",
        "We apply the neural network on Black Scholes model with Monte Carlo paths simulation. To have larger dataset, we choose to simulate 20 000 paths."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GwjYFgJsGmLq"
      },
      "source": [
        "# Function MonteCarlo Pricing\n",
        "def priceEuropeanCallMC(S0,K,r,T,sigma,M,paths):\n",
        "    \n",
        "    # generate M samples from N(0,1)\n",
        "    X = np.random.randn(paths)\n",
        "    \n",
        "    # simulate M trajectories in one step\n",
        "    ST = S0 * np.exp((r - 0.5 * sigma**2) * T + sigma * np.sqrt(T) * X)\n",
        "\n",
        "    # define payoff\n",
        "    payoff = np.where(ST < K, 0, ST - K) # acts as max(ST-K, 0) \n",
        "    \n",
        "    # MC estimate\n",
        "    discountFactor = np.exp(-r*T)\n",
        "    \n",
        "    price_MC = discountFactor*np.mean(payoff)\n",
        "    return price_MC\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WBuR81IkHfdj"
      },
      "source": [
        "# Produce 20 000 Prices Sample using MC Simulation\n",
        "\n",
        "paths = 20000\n",
        "\n",
        "data = []\n",
        "\n",
        "for n in range(nb_samples): \n",
        "    \n",
        "    sigma = np.random.uniform(low=volatilities[0], high=volatilities[-1])\n",
        "    \n",
        "    r = np.random.uniform(low=interest_rates[0], high=interest_rates[-1])\n",
        "    \n",
        "    T = np.random.uniform(low=expiry_times[0], high=expiry_times[-1])\n",
        "    \n",
        "    S0 = np.random.uniform(low=spot_prices[0], high=spot_prices[-1])\n",
        "    \n",
        "    K = np.random.uniform(low=(1-0.2)*S0, high=(1+0.2)*S0)\n",
        "    \n",
        "    M = 2000\n",
        "\n",
        "    price = priceEuropeanCallMC(S0, K, r, T, sigma, M, paths)\n",
        "\n",
        "    data.append({\n",
        "        'volatility': sigma,\n",
        "        'interest_rate': r,\n",
        "        'expiry_time': T,\n",
        "        'spot_price': S0,\n",
        "        'strike_price': K,\n",
        "        'price': price\n",
        "    })\n",
        "\n",
        "df_MC = pd.DataFrame.from_dict(data)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "09O5G-0ES1B5"
      },
      "source": [
        "# Generate x_train and y_train for Model Training\n",
        "x=df_MC[['volatility','interest_rate','expiry_time','spot_price','strike_price']].values\n",
        "y=df_MC['price'].values\n",
        "x_train, x_val, y_train, y_val=train_test_split(x,y,test_size=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MtGHATM4b4tr"
      },
      "source": [
        "The neural network model will not be changed from the above one. As a result, we don't define a new model in this case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Mv8fu0oFTDuH",
        "outputId": "efcc4c01-d19d-4ea9-f5a7-7ee47d65baa8"
      },
      "source": [
        "# Training model with MC Simulation (20 000 Option Prices Simulation with MC Sumulating 20 000 paths)\n",
        "input_dim = x_train.shape[1]\n",
        "output_dim = 1\n",
        "model = create_network(input_dim,output_dim, 200, 200)\n",
        "model.compile(optimizer='adam',loss='mean_squared_error')\n",
        "training=model.fit(x_train,y_train,epochs=200,batch_size=32,shuffle=False,validation_data=(x_val,y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 86.2502 - val_loss: 17.1830\n",
            "Epoch 2/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 32.4992 - val_loss: 14.9708\n",
            "Epoch 3/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 24.7940 - val_loss: 8.3650\n",
            "Epoch 4/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 18.1346 - val_loss: 8.8246\n",
            "Epoch 5/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 12.8777 - val_loss: 8.9332\n",
            "Epoch 6/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 10.5529 - val_loss: 9.1038\n",
            "Epoch 7/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 9.4780 - val_loss: 8.9946\n",
            "Epoch 8/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 8.7821 - val_loss: 8.8584\n",
            "Epoch 9/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 8.2847 - val_loss: 8.2438\n",
            "Epoch 10/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 7.9944 - val_loss: 7.8551\n",
            "Epoch 11/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 7.7127 - val_loss: 8.3459\n",
            "Epoch 12/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 7.5773 - val_loss: 8.0371\n",
            "Epoch 13/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 7.3568 - val_loss: 7.8090\n",
            "Epoch 14/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 7.1928 - val_loss: 7.6218\n",
            "Epoch 15/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 7.1007 - val_loss: 7.2827\n",
            "Epoch 16/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 6.9845 - val_loss: 7.5930\n",
            "Epoch 17/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 6.8964 - val_loss: 7.4842\n",
            "Epoch 18/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 6.8104 - val_loss: 7.2659\n",
            "Epoch 19/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 6.5922 - val_loss: 7.0506\n",
            "Epoch 20/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 6.4847 - val_loss: 6.6975\n",
            "Epoch 21/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 6.2984 - val_loss: 7.0147\n",
            "Epoch 22/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 6.2061 - val_loss: 6.8697\n",
            "Epoch 23/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 6.0600 - val_loss: 7.3273\n",
            "Epoch 24/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 5.9597 - val_loss: 7.5186\n",
            "Epoch 25/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 5.8142 - val_loss: 7.1706\n",
            "Epoch 26/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 5.7028 - val_loss: 6.9794\n",
            "Epoch 27/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 5.5863 - val_loss: 7.2348\n",
            "Epoch 28/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 5.4569 - val_loss: 7.1131\n",
            "Epoch 29/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 5.3557 - val_loss: 6.9860\n",
            "Epoch 30/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 5.2544 - val_loss: 6.9808\n",
            "Epoch 31/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 5.1572 - val_loss: 6.9212\n",
            "Epoch 32/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 5.0360 - val_loss: 7.0688\n",
            "Epoch 33/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 4.9122 - val_loss: 7.0346\n",
            "Epoch 34/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 4.8727 - val_loss: 6.8153\n",
            "Epoch 35/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 4.7402 - val_loss: 6.6977\n",
            "Epoch 36/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 4.6706 - val_loss: 6.9343\n",
            "Epoch 37/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 4.5702 - val_loss: 6.9318\n",
            "Epoch 38/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 4.4494 - val_loss: 6.5272\n",
            "Epoch 39/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 4.3414 - val_loss: 6.1308\n",
            "Epoch 40/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 4.1883 - val_loss: 5.6231\n",
            "Epoch 41/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 4.0883 - val_loss: 5.2764\n",
            "Epoch 42/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 3.9496 - val_loss: 5.2340\n",
            "Epoch 43/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 3.9855 - val_loss: 4.7787\n",
            "Epoch 44/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 3.7781 - val_loss: 4.8518\n",
            "Epoch 45/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 3.8133 - val_loss: 4.2289\n",
            "Epoch 46/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 3.6620 - val_loss: 4.0721\n",
            "Epoch 47/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 3.6534 - val_loss: 3.7897\n",
            "Epoch 48/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 3.6007 - val_loss: 3.6915\n",
            "Epoch 49/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 3.5011 - val_loss: 4.1574\n",
            "Epoch 50/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 3.4382 - val_loss: 3.7137\n",
            "Epoch 51/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 3.3701 - val_loss: 3.4058\n",
            "Epoch 52/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 3.3059 - val_loss: 2.8412\n",
            "Epoch 53/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 3.2307 - val_loss: 2.8495\n",
            "Epoch 54/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 3.2676 - val_loss: 2.7012\n",
            "Epoch 55/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 3.1888 - val_loss: 2.3908\n",
            "Epoch 56/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 3.1706 - val_loss: 2.3319\n",
            "Epoch 57/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 3.0533 - val_loss: 2.7063\n",
            "Epoch 58/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 3.0301 - val_loss: 2.4297\n",
            "Epoch 59/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 3.0082 - val_loss: 2.1163\n",
            "Epoch 60/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.9890 - val_loss: 2.1613\n",
            "Epoch 61/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.9228 - val_loss: 1.9001\n",
            "Epoch 62/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.8394 - val_loss: 1.6914\n",
            "Epoch 63/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.7984 - val_loss: 1.5137\n",
            "Epoch 64/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.6882 - val_loss: 1.9749\n",
            "Epoch 65/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.7635 - val_loss: 1.4830\n",
            "Epoch 66/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.6071 - val_loss: 1.3374\n",
            "Epoch 67/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.6365 - val_loss: 1.2082\n",
            "Epoch 68/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.5926 - val_loss: 1.1791\n",
            "Epoch 69/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.5416 - val_loss: 1.0588\n",
            "Epoch 70/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.5149 - val_loss: 0.9680\n",
            "Epoch 71/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.4635 - val_loss: 1.5730\n",
            "Epoch 72/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.3986 - val_loss: 1.5264\n",
            "Epoch 73/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.4052 - val_loss: 1.1114\n",
            "Epoch 74/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.3404 - val_loss: 1.6205\n",
            "Epoch 75/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.3561 - val_loss: 1.2693\n",
            "Epoch 76/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.3946 - val_loss: 1.2345\n",
            "Epoch 77/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.2610 - val_loss: 1.1788\n",
            "Epoch 78/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.2362 - val_loss: 1.5269\n",
            "Epoch 79/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.3920 - val_loss: 0.9133\n",
            "Epoch 80/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.2218 - val_loss: 1.0120\n",
            "Epoch 81/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.1571 - val_loss: 0.9759\n",
            "Epoch 82/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.1705 - val_loss: 1.2910\n",
            "Epoch 83/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.0633 - val_loss: 1.1442\n",
            "Epoch 84/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.1412 - val_loss: 1.0551\n",
            "Epoch 85/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.1288 - val_loss: 3.7326\n",
            "Epoch 86/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.5222 - val_loss: 5.0888\n",
            "Epoch 87/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.5460 - val_loss: 5.7691\n",
            "Epoch 88/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.7235 - val_loss: 5.6105\n",
            "Epoch 89/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.7210 - val_loss: 5.1102\n",
            "Epoch 90/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.4535 - val_loss: 5.5923\n",
            "Epoch 91/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.6461 - val_loss: 4.5643\n",
            "Epoch 92/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.2682 - val_loss: 3.7902\n",
            "Epoch 93/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.2365 - val_loss: 3.1494\n",
            "Epoch 94/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.1582 - val_loss: 2.0369\n",
            "Epoch 95/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.9029 - val_loss: 3.2953\n",
            "Epoch 96/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.1011 - val_loss: 5.5082\n",
            "Epoch 97/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.3757 - val_loss: 4.6063\n",
            "Epoch 98/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.3589 - val_loss: 5.7751\n",
            "Epoch 99/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.5147 - val_loss: 3.8756\n",
            "Epoch 100/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.1167 - val_loss: 5.2280\n",
            "Epoch 101/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.2479 - val_loss: 5.4063\n",
            "Epoch 102/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.2481 - val_loss: 5.5856\n",
            "Epoch 103/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.3956 - val_loss: 5.5906\n",
            "Epoch 104/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.4301 - val_loss: 4.1192\n",
            "Epoch 105/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.1089 - val_loss: 5.6108\n",
            "Epoch 106/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.4356 - val_loss: 5.6826\n",
            "Epoch 107/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.6298 - val_loss: 0.8959\n",
            "Epoch 108/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.5962 - val_loss: 4.9697\n",
            "Epoch 109/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.9852 - val_loss: 5.6488\n",
            "Epoch 110/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.0941 - val_loss: 4.5063\n",
            "Epoch 111/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.1042 - val_loss: 5.3243\n",
            "Epoch 112/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.3177 - val_loss: 5.0292\n",
            "Epoch 113/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.5650 - val_loss: 4.1545\n",
            "Epoch 114/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.9525 - val_loss: 3.3914\n",
            "Epoch 115/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.1637 - val_loss: 4.0476\n",
            "Epoch 116/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.0518 - val_loss: 4.9905\n",
            "Epoch 117/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.2791 - val_loss: 4.5713\n",
            "Epoch 118/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.0458 - val_loss: 4.6745\n",
            "Epoch 119/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.0313 - val_loss: 4.0520\n",
            "Epoch 120/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 2.1334 - val_loss: 4.7193\n",
            "Epoch 121/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.7988 - val_loss: 2.0645\n",
            "Epoch 122/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.6977 - val_loss: 3.4354\n",
            "Epoch 123/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.8153 - val_loss: 1.2877\n",
            "Epoch 124/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.6032 - val_loss: 2.0594\n",
            "Epoch 125/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.7024 - val_loss: 2.8316\n",
            "Epoch 126/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.7040 - val_loss: 0.8657\n",
            "Epoch 127/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.5476 - val_loss: 3.0708\n",
            "Epoch 128/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.6689 - val_loss: 1.7901\n",
            "Epoch 129/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.4577 - val_loss: 5.1309\n",
            "Epoch 130/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.7725 - val_loss: 0.8492\n",
            "Epoch 131/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.4731 - val_loss: 1.4124\n",
            "Epoch 132/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.5318 - val_loss: 3.4625\n",
            "Epoch 133/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.6429 - val_loss: 3.0951\n",
            "Epoch 134/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.5820 - val_loss: 4.7772\n",
            "Epoch 135/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.7888 - val_loss: 4.5618\n",
            "Epoch 136/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.6984 - val_loss: 2.9791\n",
            "Epoch 137/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.6885 - val_loss: 2.4855\n",
            "Epoch 138/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.4845 - val_loss: 5.1756\n",
            "Epoch 139/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.7780 - val_loss: 4.5242\n",
            "Epoch 140/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.7506 - val_loss: 1.4617\n",
            "Epoch 141/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.3221 - val_loss: 0.3394\n",
            "Epoch 142/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.3313 - val_loss: 2.4384\n",
            "Epoch 143/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.4673 - val_loss: 0.5697\n",
            "Epoch 144/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.3439 - val_loss: 1.4742\n",
            "Epoch 145/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.3766 - val_loss: 3.8726\n",
            "Epoch 146/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.7104 - val_loss: 2.3702\n",
            "Epoch 147/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.5401 - val_loss: 2.6123\n",
            "Epoch 148/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.5470 - val_loss: 3.2177\n",
            "Epoch 149/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.6410 - val_loss: 1.6592\n",
            "Epoch 150/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.4632 - val_loss: 1.4561\n",
            "Epoch 151/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.5472 - val_loss: 1.8525\n",
            "Epoch 152/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.5020 - val_loss: 1.0386\n",
            "Epoch 153/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.4315 - val_loss: 3.9470\n",
            "Epoch 154/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.7517 - val_loss: 1.0379\n",
            "Epoch 155/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.3994 - val_loss: 2.3348\n",
            "Epoch 156/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.4660 - val_loss: 1.3646\n",
            "Epoch 157/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.3965 - val_loss: 2.6190\n",
            "Epoch 158/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.4682 - val_loss: 1.6317\n",
            "Epoch 159/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.3267 - val_loss: 1.7777\n",
            "Epoch 160/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.3623 - val_loss: 1.1579\n",
            "Epoch 161/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.2987 - val_loss: 1.8103\n",
            "Epoch 162/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.3731 - val_loss: 1.6358\n",
            "Epoch 163/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.3587 - val_loss: 2.4012\n",
            "Epoch 164/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.4608 - val_loss: 0.4691\n",
            "Epoch 165/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.2536 - val_loss: 1.9843\n",
            "Epoch 166/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.4048 - val_loss: 2.5897\n",
            "Epoch 167/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.4914 - val_loss: 2.7640\n",
            "Epoch 168/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.4725 - val_loss: 2.4698\n",
            "Epoch 169/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.4202 - val_loss: 2.2371\n",
            "Epoch 170/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.3341 - val_loss: 1.1643\n",
            "Epoch 171/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.2648 - val_loss: 2.1905\n",
            "Epoch 172/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.4292 - val_loss: 2.6032\n",
            "Epoch 173/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.3957 - val_loss: 1.8437\n",
            "Epoch 174/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.3577 - val_loss: 2.0367\n",
            "Epoch 175/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.3673 - val_loss: 2.0225\n",
            "Epoch 176/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.3442 - val_loss: 4.3282\n",
            "Epoch 177/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.7807 - val_loss: 2.2515\n",
            "Epoch 178/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.4117 - val_loss: 1.3215\n",
            "Epoch 179/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.3239 - val_loss: 0.7942\n",
            "Epoch 180/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.1644 - val_loss: 2.4637\n",
            "Epoch 181/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.3898 - val_loss: 2.0516\n",
            "Epoch 182/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.2348 - val_loss: 3.2094\n",
            "Epoch 183/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.2825 - val_loss: 1.5894\n",
            "Epoch 184/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.2578 - val_loss: 3.9747\n",
            "Epoch 185/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.6489 - val_loss: 1.9728\n",
            "Epoch 186/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.2341 - val_loss: 1.6452\n",
            "Epoch 187/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.1788 - val_loss: 1.8086\n",
            "Epoch 188/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.1785 - val_loss: 3.9734\n",
            "Epoch 189/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.7267 - val_loss: 0.6406\n",
            "Epoch 190/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.1562 - val_loss: 3.5726\n",
            "Epoch 191/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.3818 - val_loss: 1.5671\n",
            "Epoch 192/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.2233 - val_loss: 3.4632\n",
            "Epoch 193/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.3692 - val_loss: 2.0451\n",
            "Epoch 194/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.2970 - val_loss: 0.7443\n",
            "Epoch 195/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.1619 - val_loss: 3.2240\n",
            "Epoch 196/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.4649 - val_loss: 0.9904\n",
            "Epoch 197/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.2051 - val_loss: 0.5389\n",
            "Epoch 198/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.1496 - val_loss: 1.0967\n",
            "Epoch 199/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.1705 - val_loss: 0.4256\n",
            "Epoch 200/200\n",
            "625/625 [==============================] - 1s 2ms/step - loss: 1.1871 - val_loss: 3.4599\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T692rJZTSEjn",
        "outputId": "4387f00c-5abc-4470-99ea-84453aa4c9af"
      },
      "source": [
        "# Testing Model after training with MC Silumations of 20 000 Paths\n",
        "data_test = []\n",
        "paths = 20000\n",
        "\n",
        "for n in range(num):\n",
        "    \n",
        "    sigma = np.random.uniform(low=volatilities[0], high=volatilities[-1])\n",
        "    \n",
        "    r = np.random.uniform(low=interest_rates[0], high=interest_rates[-1])\n",
        "    \n",
        "    T = np.random.uniform(low=expiry_times[0], high=expiry_times[-1])\n",
        "    \n",
        "    S0 = np.random.uniform(low=spot_prices[0], high=spot_prices[-1])\n",
        "    \n",
        "    K = np.random.uniform(low=(1-0.2)*S0, high=(1+0.2)*S0)\n",
        "    \n",
        "    M = 2000\n",
        "\n",
        "    price = priceEuropeanCallMC(S0, K, r, T, sigma, M, paths)\n",
        "\n",
        "    data.append({\n",
        "        'volatility': sigma,\n",
        "        'interest_rate': r,\n",
        "        'expiry_time': T,\n",
        "        'spot_price': S0,\n",
        "        'strike_price': K,\n",
        "        'price': price\n",
        "    })\n",
        "\n",
        "\n",
        "x_test_MC = df_MC[['volatility','interest_rate','expiry_time','spot_price','strike_price']].values\n",
        "y_test_MC = df_MC['price'].values\n",
        "\n",
        "loss = model.evaluate(x_test_MC, y_test_MC, verbose=0)\n",
        "\n",
        "print(\"average absolute error on test data is {}\".format(np.sqrt(loss)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "average absolute error on test data is 2.272863046751997\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bH2au5pjMfdO"
      },
      "source": [
        "## 3. Heston Model Pricer\n",
        "\n",
        "In this part, we will generate option price with Heston models via Monte Carlo simulation. Then, we will utilize neural network to train and test the model."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-vi5qZI_Zn0D"
      },
      "source": [
        "# Define Heston model formula\n",
        "def mc_heston(option_type,S0,K,T,V0,theta,kappa,zeta,rho,r,M):\n",
        "    \"\"\"\n",
        "    option_type:    'p' put option 'c' call option\n",
        "    S0:              the spot price of underlying stock\n",
        "    K:              the strike price\n",
        "    T:              the maturity of options\n",
        "    initial_var:    the initial value of variance\n",
        "    theta:          the long term average of price variance\n",
        "    kappa:          the mean reversion rate for the variance\n",
        "    zeta:           the volatility of volatility(the variance of the variance of stock price)\n",
        "    rho:            the correlation between the standard normal random variables W1 and W2\n",
        "    r:              the risk free rate\n",
        "    paths:          the number of repeat for monte carlo simulation\n",
        "    M:              the number of steps in each simulation\n",
        "    \"\"\"\n",
        "    dt = float(T)/float(M)\n",
        "    payoff = 0\n",
        "    S =[None] * M # Size of the series\n",
        "    S[0]=S0\n",
        "    V =[None] * M # Size of the series\n",
        "    V[0]=V0\n",
        "    \n",
        "    for t in range(1,M):\n",
        "      # generate Monte Carlo paths\n",
        "      w1 = np.random.normal(0, 1, M) \n",
        "      w2 = rho*w1+sqrt(1-rho**2)*np.random.normal(0, 1, M)\n",
        "      # volatility formula\n",
        "      V[t] = V[t-1] + kappa * (theta - V[t-1]) * dt + zeta * np.sqrt(V[t-1] * dt) * w2\n",
        "      # price formula\n",
        "      S[t] = S[t-1] * np.exp(np.sqrt(V[t-1] * dt) * w1 - V[t-1] * dt / 2)\n",
        "        \n",
        "      # option price with different types\n",
        "      if option_type == 'c':\n",
        "          payoff = np.where(S[t] < K, 0, S[t] - K) # acts as max(St-K, 0)\n",
        "      elif option_type == 'p':\n",
        "          payoff = np.where(S[t] < K, K - S[t], 0) # acts as max(K-St, 0)\n",
        "\n",
        "    return payoff"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gmu9fpvecp7E",
        "outputId": "30134fc6-3519-4127-b2b6-a1db2bc45270"
      },
      "source": [
        "# Heston option pricer test\n",
        "heston = {}\n",
        "option_type = 'c'\n",
        "S0 = 105\n",
        "K = 100\n",
        "T = 1\n",
        "V0 = 0.01\n",
        "theta = 0.01\n",
        "kappa = 2\n",
        "zeta = 0.1\n",
        "rho = 0\n",
        "r = 0.5\n",
        "M = 1000\n",
        "\n",
        "heston = {'Option Price' : mc_heston(option_type,S0,K,T,V0,theta,kappa,zeta,rho,r,M)}\n",
        "\n",
        "df_mc_heston=pd.DataFrame.from_dict(heston)\n",
        "\n",
        "MC_Heston_Simulation = np.exp(-r*T) * df_mc_heston[\"Option Price\"].mean()\n",
        "\n",
        "print(MC_Heston_Simulation)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4.208751065798564\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N-eG1Q8OqDA5"
      },
      "source": [
        "# Build input dataset\n",
        "spot_prices=[v for v in range(5,100,5)] #generate a list starting with 5 and ending with 100 with an interval of 5\n",
        "strike_prices=[u for u in range(5,90,10)]\n",
        "interest_rates=[0.01,0.015,0.02,0.025]\n",
        "volatilities=[0.05,0.1,0.15,0.2,0.25,0.3]\n",
        "expiry_times=[7,14,21,28,35,42,49]\n",
        "long_term_variances=[0.01,0.02,0.03,0.04,0.05]\n",
        "mean_reversion=[2,4,6,8,10]\n",
        "vol_of_vol=[0.1,0.2,0.3,0.4,0.5]\n",
        "corr=[-0.2,-0.1,0,0.1,0.2]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cWDw4p9GoET-",
        "outputId": "6cd4d717-026d-473d-80af-6d9c5d0a085f"
      },
      "source": [
        "# Produce 1000 Prices Sample using MC Simulation\n",
        "\n",
        "data = []\n",
        "paths = 1000\n",
        "\n",
        "for n in range(paths): \n",
        "\n",
        "    S0 = np.random.uniform(low=spot_prices[0], high=spot_prices[-1])\n",
        "\n",
        "    K = np.random.uniform(low=(1-0.2)*S0, high=(1+0.2)*S0)\n",
        "\n",
        "    T = np.random.uniform(low=expiry_times[0], high=expiry_times[-1])\n",
        "\n",
        "    V0 = np.random.uniform(low=volatilities[0], high=volatilities[-1])\n",
        "\n",
        "    theta = np.random.uniform(low=long_term_variances[0], high=long_term_variances[-1])\n",
        "\n",
        "    kappa = np.random.uniform(low=mean_reversion[0], high=mean_reversion[-1])\n",
        "\n",
        "    zeta = np.random.uniform(low=vol_of_vol[0], high=vol_of_vol[-1])\n",
        "\n",
        "    rho = np.random.uniform(low=corr[0], high=corr[-1])\n",
        "    \n",
        "    r = np.random.uniform(low=interest_rates[0], high=interest_rates[-1])\n",
        "\n",
        "    M = 1000\n",
        "\n",
        "    price=mc_heston('c',S0,K,T,V0,theta,kappa,zeta,rho,r,M).mean()\n",
        "\n",
        "\n",
        "    data.append({\n",
        "        'volatility': V0,\n",
        "        'interest_rate': r,\n",
        "        'expiry_time': T,\n",
        "        'spot_price': S0,\n",
        "        'strike_price': K,\n",
        "        'long_term_variance': theta,\n",
        "        'mean_reversion': kappa,\n",
        "        'vol_of_vol': zeta,\n",
        "        'corr': rho,\n",
        "        'price': price\n",
        "    })\n",
        "\n",
        "df_heston = pd.DataFrame.from_dict(data)\n",
        "df_heston = df_heston.fillna(0)\n",
        "\n",
        "x=df_heston[['volatility','interest_rate','expiry_time','spot_price','strike_price','long_term_variance',\n",
        "             'mean_reversion','vol_of_vol','corr']].values\n",
        "y=df_heston['price']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: RuntimeWarning: invalid value encountered in sqrt\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in sqrt\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7q08TMUPy1tW"
      },
      "source": [
        "# Generate x_train and y_train for Model Training\n",
        "x=df_heston[['volatility','interest_rate','expiry_time','spot_price','strike_price','long_term_variance',\n",
        "             'mean_reversion','vol_of_vol','corr']].values\n",
        "y=df_heston['price'].values\n",
        "x_train, x_val, y_train, y_val=train_test_split(x,y,test_size=20)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7aDeGvKpbjiE"
      },
      "source": [
        "As indicated, we need to build a new neural network model with 3 layers. Although Heston model adds volatility in its calculation, the output dimension is still 1, which is the price."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uo6NJYIezsSj"
      },
      "source": [
        "# 3 hidden layer to predict an output of 1 dimension (price and volatility) with 9 dimension input\n",
        "def create_network_heston(input_dim, output_dim, layer1_dim, layer2_dim, layer3_dim):\n",
        "    inputs=tf.keras.Input(shape=(input_dim,))\n",
        "    encoded1=tf.keras.layers.Dense(layer1_dim,activation='relu')(inputs)\n",
        "    encoded2=tf.keras.layers.Dense(layer2_dim,activation='relu')(encoded1)\n",
        "    encoded3=tf.keras.layers.Dense(layer3_dim,activation='relu')(encoded2)\n",
        "    output=tf.keras.layers.Dense(output_dim)(encoded3)\n",
        "    model_heston=tf.keras.Model(inputs,output)\n",
        "    return model_heston"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yQ_MpadvzW4V",
        "outputId": "a6203cd0-2963-4092-d8dc-4ea43eb4fd04"
      },
      "source": [
        "# Training model with MC Simulation (1000 Option Prices Simulation with MC Sumulating 200 paths)\n",
        "input_dim = x_train.shape[1]\n",
        "output_dim = 1\n",
        "model_heston = create_network_heston(input_dim,output_dim, 200, 150,150)\n",
        "model_heston.compile(optimizer='adam',loss='mean_squared_error')\n",
        "training=model_heston.fit(x_train,y_train,epochs=200,batch_size=30,shuffle=False,validation_data=(x_val,y_val))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/200\n",
            "33/33 [==============================] - 1s 7ms/step - loss: 91.0203 - val_loss: 66.5337\n",
            "Epoch 2/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 78.8994 - val_loss: 70.8798\n",
            "Epoch 3/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 79.2729 - val_loss: 69.0126\n",
            "Epoch 4/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 78.7057 - val_loss: 68.1400\n",
            "Epoch 5/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 78.5839 - val_loss: 68.7136\n",
            "Epoch 6/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 78.4059 - val_loss: 68.6271\n",
            "Epoch 7/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 78.2061 - val_loss: 68.0955\n",
            "Epoch 8/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 77.9929 - val_loss: 67.9268\n",
            "Epoch 9/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 77.6425 - val_loss: 67.3983\n",
            "Epoch 10/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 77.4703 - val_loss: 68.1339\n",
            "Epoch 11/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 77.3609 - val_loss: 66.6348\n",
            "Epoch 12/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 77.1893 - val_loss: 66.6358\n",
            "Epoch 13/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 76.8819 - val_loss: 65.8525\n",
            "Epoch 14/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 76.8546 - val_loss: 66.1212\n",
            "Epoch 15/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 77.0086 - val_loss: 65.8973\n",
            "Epoch 16/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 76.6525 - val_loss: 65.4662\n",
            "Epoch 17/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 76.4835 - val_loss: 65.1680\n",
            "Epoch 18/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 76.0790 - val_loss: 65.3920\n",
            "Epoch 19/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 76.0440 - val_loss: 64.1621\n",
            "Epoch 20/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 75.6193 - val_loss: 63.5713\n",
            "Epoch 21/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 75.2907 - val_loss: 64.4154\n",
            "Epoch 22/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 75.1203 - val_loss: 62.8196\n",
            "Epoch 23/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 74.5353 - val_loss: 62.7450\n",
            "Epoch 24/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 74.4497 - val_loss: 63.4291\n",
            "Epoch 25/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 73.9021 - val_loss: 62.4372\n",
            "Epoch 26/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 73.5418 - val_loss: 62.5043\n",
            "Epoch 27/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 73.2272 - val_loss: 60.7217\n",
            "Epoch 28/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 72.2548 - val_loss: 58.8761\n",
            "Epoch 29/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 71.1077 - val_loss: 57.5315\n",
            "Epoch 30/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 70.0699 - val_loss: 55.3505\n",
            "Epoch 31/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 69.3504 - val_loss: 55.0598\n",
            "Epoch 32/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 68.7367 - val_loss: 52.6864\n",
            "Epoch 33/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 67.9872 - val_loss: 51.1222\n",
            "Epoch 34/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 67.4948 - val_loss: 51.2694\n",
            "Epoch 35/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 66.1450 - val_loss: 48.2847\n",
            "Epoch 36/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 65.2715 - val_loss: 47.0826\n",
            "Epoch 37/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 64.2404 - val_loss: 45.2954\n",
            "Epoch 38/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 62.7112 - val_loss: 43.3132\n",
            "Epoch 39/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 62.0121 - val_loss: 41.8564\n",
            "Epoch 40/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 60.9272 - val_loss: 39.4427\n",
            "Epoch 41/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 59.4947 - val_loss: 38.8171\n",
            "Epoch 42/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 58.3589 - val_loss: 37.8433\n",
            "Epoch 43/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 57.2061 - val_loss: 36.0705\n",
            "Epoch 44/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 56.0283 - val_loss: 35.0800\n",
            "Epoch 45/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 54.6504 - val_loss: 33.9540\n",
            "Epoch 46/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 53.5325 - val_loss: 32.9662\n",
            "Epoch 47/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 52.5452 - val_loss: 31.7876\n",
            "Epoch 48/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 51.4592 - val_loss: 30.4801\n",
            "Epoch 49/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 50.4121 - val_loss: 29.2891\n",
            "Epoch 50/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 49.3374 - val_loss: 28.2092\n",
            "Epoch 51/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 47.9017 - val_loss: 28.6796\n",
            "Epoch 52/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 46.8909 - val_loss: 27.8914\n",
            "Epoch 53/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 46.0800 - val_loss: 28.6517\n",
            "Epoch 54/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 44.8976 - val_loss: 28.6998\n",
            "Epoch 55/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 43.7435 - val_loss: 29.6475\n",
            "Epoch 56/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 43.0881 - val_loss: 29.8642\n",
            "Epoch 57/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 42.0038 - val_loss: 31.5150\n",
            "Epoch 58/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 41.8944 - val_loss: 29.9730\n",
            "Epoch 59/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 41.3372 - val_loss: 29.9252\n",
            "Epoch 60/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 41.0544 - val_loss: 29.2928\n",
            "Epoch 61/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 41.0765 - val_loss: 26.6168\n",
            "Epoch 62/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 39.9319 - val_loss: 25.5477\n",
            "Epoch 63/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 39.4364 - val_loss: 25.9259\n",
            "Epoch 64/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 38.2649 - val_loss: 25.2642\n",
            "Epoch 65/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 37.8095 - val_loss: 25.0141\n",
            "Epoch 66/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 37.3107 - val_loss: 24.4823\n",
            "Epoch 67/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 36.8264 - val_loss: 24.4765\n",
            "Epoch 68/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 36.0126 - val_loss: 24.9221\n",
            "Epoch 69/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 35.6029 - val_loss: 25.6936\n",
            "Epoch 70/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 35.0522 - val_loss: 24.8446\n",
            "Epoch 71/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 34.6646 - val_loss: 25.4986\n",
            "Epoch 72/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 34.1447 - val_loss: 25.7989\n",
            "Epoch 73/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 34.0813 - val_loss: 25.8473\n",
            "Epoch 74/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 33.7392 - val_loss: 26.1301\n",
            "Epoch 75/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 33.6049 - val_loss: 25.5979\n",
            "Epoch 76/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 33.8222 - val_loss: 25.0754\n",
            "Epoch 77/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 34.3172 - val_loss: 25.8514\n",
            "Epoch 78/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 34.5699 - val_loss: 27.3771\n",
            "Epoch 79/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 36.1570 - val_loss: 33.6686\n",
            "Epoch 80/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 37.3472 - val_loss: 35.6904\n",
            "Epoch 81/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 36.2338 - val_loss: 32.8409\n",
            "Epoch 82/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 33.7587 - val_loss: 32.0827\n",
            "Epoch 83/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 32.3917 - val_loss: 28.7433\n",
            "Epoch 84/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 31.0940 - val_loss: 30.4393\n",
            "Epoch 85/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 30.8480 - val_loss: 30.0438\n",
            "Epoch 86/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 29.9581 - val_loss: 28.6179\n",
            "Epoch 87/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 29.8347 - val_loss: 28.3289\n",
            "Epoch 88/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 28.4180 - val_loss: 28.6113\n",
            "Epoch 89/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 27.5464 - val_loss: 27.9853\n",
            "Epoch 90/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 27.8276 - val_loss: 29.3503\n",
            "Epoch 91/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 27.4905 - val_loss: 27.5407\n",
            "Epoch 92/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 26.4990 - val_loss: 29.3802\n",
            "Epoch 93/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 26.5209 - val_loss: 29.0536\n",
            "Epoch 94/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 25.7416 - val_loss: 28.1964\n",
            "Epoch 95/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 25.4285 - val_loss: 28.7101\n",
            "Epoch 96/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 25.1681 - val_loss: 29.6912\n",
            "Epoch 97/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 25.1742 - val_loss: 28.7196\n",
            "Epoch 98/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 25.4010 - val_loss: 28.0498\n",
            "Epoch 99/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 24.6624 - val_loss: 28.9069\n",
            "Epoch 100/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 24.3734 - val_loss: 28.5303\n",
            "Epoch 101/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 24.4349 - val_loss: 29.0798\n",
            "Epoch 102/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 24.3923 - val_loss: 28.7292\n",
            "Epoch 103/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 23.7966 - val_loss: 28.0712\n",
            "Epoch 104/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 23.7712 - val_loss: 29.7406\n",
            "Epoch 105/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 22.9735 - val_loss: 28.8592\n",
            "Epoch 106/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 23.1559 - val_loss: 28.7672\n",
            "Epoch 107/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 23.5332 - val_loss: 30.1926\n",
            "Epoch 108/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 23.3169 - val_loss: 28.6586\n",
            "Epoch 109/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 22.9146 - val_loss: 29.7553\n",
            "Epoch 110/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 22.9780 - val_loss: 28.7348\n",
            "Epoch 111/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 23.2483 - val_loss: 28.6313\n",
            "Epoch 112/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 23.0732 - val_loss: 30.1270\n",
            "Epoch 113/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 23.1725 - val_loss: 30.1102\n",
            "Epoch 114/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 23.0871 - val_loss: 32.7528\n",
            "Epoch 115/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 23.6318 - val_loss: 28.3650\n",
            "Epoch 116/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 22.1215 - val_loss: 32.0996\n",
            "Epoch 117/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 23.0848 - val_loss: 29.4996\n",
            "Epoch 118/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 23.3341 - val_loss: 29.4968\n",
            "Epoch 119/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 23.0135 - val_loss: 29.9750\n",
            "Epoch 120/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 24.6337 - val_loss: 32.3687\n",
            "Epoch 121/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 26.8401 - val_loss: 37.5786\n",
            "Epoch 122/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 34.3972 - val_loss: 55.1919\n",
            "Epoch 123/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 40.3210 - val_loss: 52.8143\n",
            "Epoch 124/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 30.9788 - val_loss: 51.4581\n",
            "Epoch 125/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 28.2417 - val_loss: 47.9891\n",
            "Epoch 126/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 26.7071 - val_loss: 50.4339\n",
            "Epoch 127/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 27.3950 - val_loss: 53.5351\n",
            "Epoch 128/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 26.3582 - val_loss: 55.5173\n",
            "Epoch 129/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 26.7438 - val_loss: 49.3156\n",
            "Epoch 130/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 27.0979 - val_loss: 56.3466\n",
            "Epoch 131/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 27.9423 - val_loss: 48.6650\n",
            "Epoch 132/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 27.4906 - val_loss: 39.4468\n",
            "Epoch 133/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 30.3716 - val_loss: 36.4075\n",
            "Epoch 134/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 28.1590 - val_loss: 34.8373\n",
            "Epoch 135/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 26.1758 - val_loss: 33.5705\n",
            "Epoch 136/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 25.4067 - val_loss: 36.9752\n",
            "Epoch 137/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 23.7783 - val_loss: 37.6120\n",
            "Epoch 138/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 23.0774 - val_loss: 32.7721\n",
            "Epoch 139/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 23.6383 - val_loss: 36.6967\n",
            "Epoch 140/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 23.6470 - val_loss: 34.3159\n",
            "Epoch 141/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 22.0143 - val_loss: 35.9435\n",
            "Epoch 142/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 22.5225 - val_loss: 34.5729\n",
            "Epoch 143/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 22.4272 - val_loss: 36.4964\n",
            "Epoch 144/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 22.1469 - val_loss: 36.9885\n",
            "Epoch 145/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 21.6984 - val_loss: 37.7880\n",
            "Epoch 146/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 21.3383 - val_loss: 36.8768\n",
            "Epoch 147/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 21.0243 - val_loss: 38.3128\n",
            "Epoch 148/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 20.8454 - val_loss: 39.6422\n",
            "Epoch 149/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 21.5180 - val_loss: 38.4439\n",
            "Epoch 150/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 21.8540 - val_loss: 39.9243\n",
            "Epoch 151/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 20.7370 - val_loss: 40.5949\n",
            "Epoch 152/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 21.1946 - val_loss: 35.8213\n",
            "Epoch 153/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 21.1865 - val_loss: 39.9681\n",
            "Epoch 154/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 21.6375 - val_loss: 37.5347\n",
            "Epoch 155/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 21.0107 - val_loss: 39.2203\n",
            "Epoch 156/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 20.5659 - val_loss: 36.9763\n",
            "Epoch 157/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 21.0243 - val_loss: 34.5986\n",
            "Epoch 158/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 20.2054 - val_loss: 39.6269\n",
            "Epoch 159/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 20.0078 - val_loss: 35.0391\n",
            "Epoch 160/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 20.3995 - val_loss: 41.3293\n",
            "Epoch 161/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 20.3135 - val_loss: 38.0829\n",
            "Epoch 162/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 19.5915 - val_loss: 40.7364\n",
            "Epoch 163/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 20.0522 - val_loss: 40.1875\n",
            "Epoch 164/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 19.1551 - val_loss: 40.4917\n",
            "Epoch 165/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 19.6548 - val_loss: 40.1337\n",
            "Epoch 166/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 19.8256 - val_loss: 42.6384\n",
            "Epoch 167/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 19.3017 - val_loss: 44.6436\n",
            "Epoch 168/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 19.1424 - val_loss: 42.4445\n",
            "Epoch 169/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 18.6156 - val_loss: 42.1286\n",
            "Epoch 170/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 19.9916 - val_loss: 43.5289\n",
            "Epoch 171/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 19.9056 - val_loss: 42.7298\n",
            "Epoch 172/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 19.3474 - val_loss: 46.2879\n",
            "Epoch 173/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 18.4308 - val_loss: 47.4570\n",
            "Epoch 174/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 18.2928 - val_loss: 45.2599\n",
            "Epoch 175/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 19.3915 - val_loss: 49.9533\n",
            "Epoch 176/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 19.7506 - val_loss: 51.2883\n",
            "Epoch 177/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 19.0133 - val_loss: 45.9152\n",
            "Epoch 178/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 18.4338 - val_loss: 45.9510\n",
            "Epoch 179/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 19.0502 - val_loss: 43.9892\n",
            "Epoch 180/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 28.5834 - val_loss: 54.4066\n",
            "Epoch 181/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 29.0894 - val_loss: 46.7987\n",
            "Epoch 182/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 30.6970 - val_loss: 55.3304\n",
            "Epoch 183/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 45.7821 - val_loss: 52.0504\n",
            "Epoch 184/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 42.4300 - val_loss: 49.0759\n",
            "Epoch 185/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 36.0049 - val_loss: 52.7958\n",
            "Epoch 186/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 38.2517 - val_loss: 54.8775\n",
            "Epoch 187/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 41.4348 - val_loss: 54.4046\n",
            "Epoch 188/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 44.1086 - val_loss: 47.0520\n",
            "Epoch 189/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 40.1726 - val_loss: 39.4350\n",
            "Epoch 190/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 31.7545 - val_loss: 40.0431\n",
            "Epoch 191/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 26.0303 - val_loss: 36.5448\n",
            "Epoch 192/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 24.1953 - val_loss: 44.3931\n",
            "Epoch 193/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 25.7751 - val_loss: 49.6980\n",
            "Epoch 194/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 25.1191 - val_loss: 49.9400\n",
            "Epoch 195/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 26.0344 - val_loss: 52.2144\n",
            "Epoch 196/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 27.4023 - val_loss: 51.9892\n",
            "Epoch 197/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 30.7120 - val_loss: 59.4875\n",
            "Epoch 198/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 54.6756 - val_loss: 30.9933\n",
            "Epoch 199/200\n",
            "33/33 [==============================] - 0s 4ms/step - loss: 32.9433 - val_loss: 38.9581\n",
            "Epoch 200/200\n",
            "33/33 [==============================] - 0s 3ms/step - loss: 31.3135 - val_loss: 52.6260\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hmYw3XX2Hbkt",
        "outputId": "83aaf502-3840-48f3-bfdc-d735ed31fd7b"
      },
      "source": [
        "# Testing Model after training with MC Silumations of 1000 Paths\n",
        "data = []\n",
        "paths = 1000\n",
        "\n",
        "for n in range(paths): \n",
        "\n",
        "    S0 = np.random.uniform(low=spot_prices[0], high=spot_prices[-1])\n",
        "\n",
        "    K = np.random.uniform(low=(1-0.2)*S0, high=(1+0.2)*S0)\n",
        "\n",
        "    T = np.random.uniform(low=expiry_times[0], high=expiry_times[-1])\n",
        "\n",
        "    V0 = np.random.uniform(low=volatilities[0], high=volatilities[-1])\n",
        "\n",
        "    theta = np.random.uniform(low=long_term_variances[0], high=long_term_variances[-1])\n",
        "\n",
        "    kappa = np.random.uniform(low=mean_reversion[0], high=mean_reversion[-1])\n",
        "\n",
        "    zeta = np.random.uniform(low=vol_of_vol[0], high=vol_of_vol[-1])\n",
        "\n",
        "    rho = np.random.uniform(low=corr[0], high=corr[-1])\n",
        "    \n",
        "    r = np.random.uniform(low=interest_rates[0], high=interest_rates[-1])\n",
        "\n",
        "    M = 1000\n",
        "\n",
        "    price=mc_heston('c',S0,K,T,V0,theta,kappa,zeta,rho,r,M).mean()\n",
        "\n",
        "\n",
        "    data.append({\n",
        "        'volatility': V0,\n",
        "        'interest_rate': r,\n",
        "        'expiry_time': T,\n",
        "        'spot_price': S0,\n",
        "        'strike_price': K,\n",
        "        'long_term_variance': theta,\n",
        "        'mean_reversion': kappa,\n",
        "        'vol_of_vol': zeta,\n",
        "        'corr': rho,\n",
        "        'price': price\n",
        "    })\n",
        "\n",
        "df_heston = pd.DataFrame.from_dict(data)\n",
        "df_heston = df_heston.fillna(0)\n",
        "\n",
        "\n",
        "x_test_heston = df_heston[['volatility','interest_rate','expiry_time','spot_price','strike_price','long_term_variance',\n",
        "             'mean_reversion','vol_of_vol','corr']].values\n",
        "y_test_heston = df_heston['price'].values\n",
        "\n",
        "loss_heston = model_heston.evaluate(x_test_heston, y_test_heston, verbose=0)\n",
        "\n",
        "print(\"average absolute error on test data for Heston model is {}\".format(np.sqrt(loss_heston)))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:29: RuntimeWarning: invalid value encountered in sqrt\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:31: RuntimeWarning: invalid value encountered in sqrt\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "average absolute error on test data for Heston model is 7.606948747723248\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TQ6Q1gHIcDtj"
      },
      "source": [
        "# Conclusion\n",
        "From the above three different models, we note that the analytical option pricing tool has the lowest MSE result; the Black Scholes model follow tightly and the last one is Heston model. \n",
        "\n",
        "However, we cannot conclude that our neural network model doesn't work well on Heston model, since it has 3 layers while the other two has two layers. We have tried the Heston model with a neural network of 2 layers, the result is much better. Therefore, we can see that different layers in neural network system can be vital to the result.\n",
        "\n",
        "We can conclude that neural network models are really sensitive to their hyperparameters. We should try various hyperparameters to find the most optimised ones. Since the selection process can be long and random, we choose not to show in this notebook.\n",
        "\n",
        "In addition, we have set up \"shuffle\" to \"False\" in all 3 models. As a result, the random possibilities might be biased in our traing set.\n",
        "\n"
      ]
    }
  ]
}